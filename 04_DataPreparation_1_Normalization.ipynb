{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC529: 04_DataPreparation_1_Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy\n",
    "\n",
    "# to display nice model diagram\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Original data\n",
      "[[192.]\n",
      " [197.]\n",
      " [192.]\n",
      " [182.]\n",
      " [206.]\n",
      " [192.]\n",
      " [190.]\n",
      " [178.]\n",
      " [196.]\n",
      " [201.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# separate array into input and output components\n",
    "X = numpy.array([192,197,192,182,206,192,190,178,196,201], float).reshape(-1,1)\n",
    "print(X.shape)\n",
    "#numpy.set_printoptions(precision=0)\n",
    "numpy.set_printoptions(suppress=True) \n",
    "\n",
    "print (\"Original data\")\n",
    "print(X, \"\\n\")\t# read the frist rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standardization and Range Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Range normalization [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescale data (between 0 and 1)\n",
      "MinMaxScaler()\n",
      "[[0.5       ]\n",
      " [0.67857143]\n",
      " [0.5       ]\n",
      " [0.14285714]\n",
      " [1.        ]] \n",
      "\n",
      "Rescale data (between 0 and 1)\n",
      "MinMaxScaler()\n",
      "[[0.5       ]\n",
      " [0.67857143]\n",
      " [0.5       ]\n",
      " [0.14285714]\n",
      " [1.        ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "print (\"Rescale data (between 0 and 1)\")\n",
    "print (scaler) \n",
    "print(rescaledX[:5], \"\\n\")\n",
    "\n",
    "# option2: fit and then transform\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler2.fit(X)\n",
    "rescaledX2 = scaler2.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "print (\"Rescale data (between 0 and 1)\")\n",
    "print (scaler2) \n",
    "print(rescaledX2[:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing._data.MinMaxScaler"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Range normalization [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescale data (between -1 and 1)\n",
      "MinMaxScaler(feature_range=(-1, 1))\n",
      "[[ 0.        ]\n",
      " [ 0.35714286]\n",
      " [ 0.        ]\n",
      " [-0.71428571]\n",
      " [ 1.        ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler3 = MinMaxScaler(feature_range=(-1, 1))\n",
    "rescaledX3 = scaler3.fit_transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "print (\"Rescale data (between -1 and 1)\")\n",
    "print (scaler3)\n",
    "print(rescaledX3[0:5], \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Standardization (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardize data (0 mean, 1 stdev)\n",
      "StandardScaler()\n",
      "[[-0.07654655]\n",
      " [ 0.5613414 ]\n",
      " [-0.07654655]\n",
      " [-1.35232246]\n",
      " [ 1.70953972]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Standardize data (0 mean, 1 stdev)\n",
    "################################################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler4 = StandardScaler().fit(X)\n",
    "rescaledX4 = scaler4.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "print (\"Standardize data (0 mean, 1 stdev)\")\n",
    "print (scaler4)\n",
    "print(rescaledX4[0:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Parameters and Attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 {color: black;background-color: white;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 pre{padding: 0;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-toggleable {background-color: white;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-item {z-index: 1;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-parallel-item:only-child::after {width: 0;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-65e9c98f-c8ab-458c-bfee-bf9682262ea3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"69d51ae7-5fde-4b83-8574-c29cd263ab3d\" type=\"checkbox\" checked><label for=\"69d51ae7-5fde-4b83-8574-c29cd263ab3d\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'with_mean': True,\n",
       " 'with_std': True,\n",
       " 'copy': True,\n",
       " 'n_features_in_': 1,\n",
       " 'n_samples_seen_': 10,\n",
       " 'mean_': array([192.6]),\n",
       " 'var_': array([61.44]),\n",
       " 'scale_': array([7.83836718])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dictionary of model parameters and attributes\n",
    "vars(scaler4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('with_mean', True), ('with_std', True), ('copy', True), ('n_features_in_', 1), ('n_samples_seen_', 10), ('mean_', array([192.6])), ('var_', array([61.44])), ('scale_', array([7.838]))]) \n",
      "\n",
      "dict_keys(['with_mean', 'with_std', 'copy', 'n_features_in_', 'n_samples_seen_', 'mean_', 'var_', 'scale_']) \n",
      "\n",
      "{'with_mean': True, 'with_std': True, 'copy': True, 'n_features_in_': 1, 'n_samples_seen_': 10, 'mean_': array([192.6]), 'var_': array([61.44]), 'scale_': array([7.838])} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vars(scaler4).items(),\"\\n\")\n",
    "print(vars(scaler4).keys(),\"\\n\")\n",
    "print(scaler4.__dict__,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with_mean',\n",
       " 'with_std',\n",
       " 'copy',\n",
       " 'n_features_in_',\n",
       " 'n_samples_seen_',\n",
       " 'mean_',\n",
       " 'var_',\n",
       " 'scale_']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way access model parameters and attributes\n",
    "def get_properies_all(model):\n",
    "    return [i for i in model.__dict__]\n",
    "get_properies_all(scaler4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n_features_in_', 'n_samples_seen_', 'mean_', 'var_', 'scale_']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all attributes (**attributes end with _**)\n",
    "def get_properies(model):\n",
    "    return [i for i in model.__dict__ if i.endswith(\"_\")]\n",
    "get_properies(scaler4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_features_in_', 'n_samples_seen_', 'scale_', 'min_', 'data_min_', 'data_max_', 'data_range_']\n",
      "['n_features_in_', 'n_samples_seen_', 'mean_', 'var_', 'scale_']\n"
     ]
    }
   ],
   "source": [
    "print(get_properies(scaler3)) # minMaxScaler\n",
    "print(get_properies(scaler4)) # standardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      n_features_in_:\t1\n",
      "     n_samples_seen_:\t10\n",
      "               mean_:\t[192.6]\n",
      "                var_:\t[61.44]\n",
      "              scale_:\t[7.83836718]\n"
     ]
    }
   ],
   "source": [
    "for key, val in vars(scaler4).items():\n",
    "    if key.endswith(\"_\"):\n",
    "        print('{:>20s}:\\t{}'.format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min/Max/Mean/variance/STD of feature values\n",
      "[178.]\n",
      "[206.]\n",
      "[192.6]\n",
      "[61.44]\n",
      "[7.838]\n"
     ]
    }
   ],
   "source": [
    "# double check with numpy solution\n",
    "numpy.set_printoptions(precision=3)\n",
    "\n",
    "# Basic statistics\n",
    "print (\"Min/Max/Mean/variance/STD of feature values\")\n",
    "print (numpy.min(X, axis=0)) # mins\n",
    "print (numpy.max(X, axis=0)) # max \n",
    "print (numpy.mean(X, axis=0))# means \n",
    "print (numpy.var(X, axis=0)) # variance \n",
    "print (numpy.std(X, axis=0)) # std \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discretization\n",
    "\n",
    "https://www.journaldev.com/54363/data-discretization-python-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 quantile transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [[192], [197], [192], [182], [206], [192], [190], [178], [196], [201]]\n",
      "Quantile:  [[1], [2], [1], [0], [2], [1], [0], [0], [2], [2]]\n"
     ]
    }
   ],
   "source": [
    "# Import the class\n",
    "# ‘quantile’: All bins in each feature have the same number of points.\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    " \n",
    "#Discrete the data\n",
    "transf = KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy = 'quantile')\n",
    " \n",
    "#fit transform \n",
    "X_q = transf.fit_transform(X)\n",
    "print(\"Original: \", X.astype(int).tolist())\n",
    "print(\"Quantile: \", X_q.astype(int).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 uniform transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [[192], [197], [192], [182], [206], [192], [190], [178], [196], [201]]\n",
      "Quantile:  [[1], [2], [1], [0], [2], [1], [0], [0], [2], [2]]\n",
      "Uniform:   [[1], [2], [1], [0], [2], [1], [1], [0], [1], [2]]\n"
     ]
    }
   ],
   "source": [
    "# Import the class\n",
    "# uniform’: All bins in each feature have identical widths.\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Discrete the data\n",
    "transf = KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy = 'uniform')\n",
    " \n",
    "# fit transform \n",
    "X_u = transf.fit_transform(X)\n",
    "print(\"Original: \", X.astype(int).tolist())\n",
    "print(\"Quantile: \", X_q.astype(int).tolist())\n",
    "print(\"Uniform:  \", X_u.astype(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Kmeans transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fe1a36d3310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fe1a36d3310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/home/dche/anaconda3/lib/python3.9/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [[192], [197], [192], [182], [206], [192], [190], [178], [196], [201]]\n",
      "Quantile:  [[1], [2], [1], [0], [2], [1], [0], [0], [2], [2]]\n",
      "Uniform:   [[1], [2], [1], [0], [2], [1], [1], [0], [1], [2]]\n",
      "Kmeans:    [[1], [2], [1], [0], [2], [1], [1], [0], [1], [2]]\n"
     ]
    }
   ],
   "source": [
    "#Import the class\n",
    "#‘kmeans’: Values in each bin have the same nearest center of a 1D k-means cluster.\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    " \n",
    "#Discrete the data\n",
    "transf = KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy = 'kmeans')\n",
    " \n",
    "#fit transform \n",
    "X_kmeans = transf.fit_transform(X)\n",
    "print(\"Original: \", X.astype(int).tolist())\n",
    "print(\"Quantile: \", X_q.astype(int).tolist())\n",
    "print(\"Uniform:  \", X_u.astype(int).tolist())\n",
    "print(\"Kmeans:   \",  X_kmeans.astype(int).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unit Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize data (length of 1)\n",
      "[[0.267 0.535 0.802]]\n",
      "[[0.267 0.535 0.802]]\n",
      "[[0.535 0.802 0.267]]\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Normalize data (length of 1)\n",
    "#############################################\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "U1 = np.array([[1,2,3]])\n",
    "U2 = np.array([[2,4,6]])\n",
    "U3 = np.array([[2,3,1]])\n",
    "\n",
    "norm_u1 = Normalizer().fit_transform(U1)\n",
    "norm_u2 = Normalizer().fit_transform(U2)\n",
    "norm_u3 = Normalizer().fit_transform(U3)\n",
    "\n",
    "# summarize transformed data\n",
    "print (\"Normalize data (length of 1)\")\n",
    "print(norm_u1)\n",
    "print(norm_u2)\n",
    "print(norm_u3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
